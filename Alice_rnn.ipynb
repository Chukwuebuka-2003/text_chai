{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9352009,"sourceType":"datasetVersion","datasetId":5668993}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-14T16:58:38.961134Z","iopub.execute_input":"2024-09-14T16:58:38.961402Z","iopub.status.idle":"2024-09-14T16:58:39.399073Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"/kaggle/input/rnn-char/alice.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RNN","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\n# Load and preprocess data\nwith open(\"/kaggle/input/rnn-char/alice.txt\", 'r', encoding='utf-8') as f:\n    text = f.read().lower()\n\n# Create character to integer and integer to character mappings\nchars = sorted(set(text))\nchar_to_int = {c: i for i, c in enumerate(chars)}\nint_to_char = {i: c for i, c in enumerate(chars)}\n\n# Encode text as integers\nencoded_text = [char_to_int[c] for c in text]\n\n# Hyperparameters\nseq_length = 100\nbatch_size = 64\nn_epochs = 50\nlearning_rate = 0.003\n\n# Create sequences\ndef create_sequences(data, seq_length):\n    sequences = []\n    targets = []\n    for i in range(0, len(data) - seq_length):\n        sequences.append(data[i:i + seq_length])\n        targets.append(data[i + seq_length])\n    return np.array(sequences), np.array(targets)\n\nX, y = create_sequences(encoded_text, seq_length)\n\n# Convert to tensors\nX = torch.tensor(X, dtype=torch.long)\ny = torch.tensor(y, dtype=torch.long)\n\n# DataLoader\ndataset = torch.utils.data.TensorDataset(X, y)\nloader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T16:58:39.400572Z","iopub.execute_input":"2024-09-14T16:58:39.401076Z","iopub.status.idle":"2024-09-14T16:58:41.442381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the RNN Model\nclass CharRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n        super(CharRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden):\n        x = self.embedding(x)\n        out, hidden = self.rnn(x, hidden)\n        out = self.fc(out[:, -1])\n        return out, hidden\n\n    def init_hidden(self, batch_size):\n        return torch.zeros(self.n_layers, batch_size, self.hidden_size)\n\n# Model parameters\ninput_size = len(chars)\nhidden_size = 256\noutput_size = len(chars)\nn_layers = 2\n\n# Initialize the model\nmodel = CharRNN(input_size, hidden_size, output_size, n_layers)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()  # CrossEntropy for character prediction\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T16:58:41.443388Z","iopub.execute_input":"2024-09-14T16:58:41.443696Z","iopub.status.idle":"2024-09-14T16:58:41.462463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training function\ndef train_model(model, loader, n_epochs=n_epochs):\n    model.train()\n    for epoch in range(n_epochs):\n        epoch_loss = 0\n        \n        for X_batch, y_batch in loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            \n            # Initialize hidden state with the current batch size\n            batch_size = X_batch.size(0)\n            hidden = model.init_hidden(batch_size).to(device)\n            \n            # Detach hidden state to prevent backprop through previous steps\n            hidden = hidden.detach()\n            \n            # Zero the gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            output, hidden = model(X_batch, hidden)\n            \n            # Compute loss\n            loss = criterion(output, y_batch)\n            \n            # Backpropagation and optimization\n            loss.backward()\n            optimizer.step()\n            \n            # Accumulate loss\n            epoch_loss += loss.item()\n\n        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss / len(loader):.4f}')\n\n# Train the model\ntrain_model(model, loader, n_epochs=n_epochs)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T16:58:41.465686Z","iopub.execute_input":"2024-09-14T16:58:41.466116Z","iopub.status.idle":"2024-09-14T17:11:19.774379Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50, Loss: 1.9705\nEpoch 2/50, Loss: 1.8259\nEpoch 3/50, Loss: 1.8048\nEpoch 4/50, Loss: 1.7981\nEpoch 5/50, Loss: 1.7975\nEpoch 6/50, Loss: 1.7986\nEpoch 7/50, Loss: 1.8114\nEpoch 8/50, Loss: 1.8168\nEpoch 9/50, Loss: 1.8272\nEpoch 10/50, Loss: 1.8243\nEpoch 11/50, Loss: 1.8222\nEpoch 12/50, Loss: 1.8303\nEpoch 13/50, Loss: 1.8352\nEpoch 14/50, Loss: 1.8469\nEpoch 15/50, Loss: 1.8581\nEpoch 16/50, Loss: 1.8720\nEpoch 17/50, Loss: 1.8762\nEpoch 18/50, Loss: 1.8898\nEpoch 19/50, Loss: 1.8808\nEpoch 20/50, Loss: 1.8962\nEpoch 21/50, Loss: 1.9345\nEpoch 22/50, Loss: 1.9200\nEpoch 23/50, Loss: 1.9079\nEpoch 24/50, Loss: 1.9027\nEpoch 25/50, Loss: 1.9122\nEpoch 26/50, Loss: 1.9215\nEpoch 27/50, Loss: 1.9256\nEpoch 28/50, Loss: 1.9348\nEpoch 29/50, Loss: 1.9213\nEpoch 30/50, Loss: 1.9255\nEpoch 31/50, Loss: 1.9357\nEpoch 32/50, Loss: 1.9122\nEpoch 33/50, Loss: 1.9201\nEpoch 34/50, Loss: 1.9418\nEpoch 35/50, Loss: 1.9327\nEpoch 36/50, Loss: 1.9307\nEpoch 37/50, Loss: 1.9347\nEpoch 38/50, Loss: 1.9537\nEpoch 39/50, Loss: 2.0042\nEpoch 40/50, Loss: 1.9822\nEpoch 41/50, Loss: 1.9838\nEpoch 42/50, Loss: 1.9832\nEpoch 43/50, Loss: 1.9883\nEpoch 44/50, Loss: 1.9893\nEpoch 45/50, Loss: 2.0196\nEpoch 46/50, Loss: 2.0088\nEpoch 47/50, Loss: 2.0050\nEpoch 48/50, Loss: 1.9922\nEpoch 49/50, Loss: 2.0125\nEpoch 50/50, Loss: 1.9865\n","output_type":"stream"}]},{"cell_type":"code","source":"# Text generation with temperature\ndef generate_text(model, start_str, gen_length=1000, temperature=0.01):\n    model.eval()\n    hidden = model.init_hidden(1).to(device)\n    input = torch.tensor([char_to_int[ch] for ch in start_str], dtype=torch.long).unsqueeze(0).to(device)\n    generated_text = start_str\n\n    for _ in range(gen_length):\n        output, hidden = model(input, hidden)\n        output = output.div(temperature).exp()\n        char_idx = torch.multinomial(output, 1).item()\n        char = int_to_char[char_idx]\n        generated_text += char\n        input = torch.tensor([[char_idx]], dtype=torch.long).to(device)\n    \n    return generated_text\n\n# Generate text from a seed\nseed_text = \"alice was beginning to get very tired\"\nprint(generate_text(model, seed_text, gen_length=1000, temperature=0.654321))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:24:49.356451Z","iopub.execute_input":"2024-09-14T17:24:49.357122Z","iopub.status.idle":"2024-09-14T17:24:50.010391Z","shell.execute_reply.started":"2024-09-14T17:24:49.357081Z","shell.execute_reply":"2024-09-14T17:24:50.009458Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"alice was beginning to get very tired of cours lirhat the waster qurt, and voin a casenber.\n\nthe use fighin.\n\nthe had mistent therum of sork this grown alice offer the courd that could a she exritle of court the rards of the she con’t the cound this the out that she rermire ontion she cours at sorge was of that of this to frong thee. shis the from this from that it and and thing alice. “as of spat it and far a lith arrice, agair of the or list and ter ond a she like caser my shis a lith\nof such that lith in a casth a the mat sithire liked of thoudn’t the elor as ond the juring her offure the the right this grick i thrill you fastory a lirher havent the reav asrise of thill the casting varch you as rarrioustion. and the roest ass very of making the waster with and si a stlar you mare mi wast a roes a ver lird of the was of this office office, and the can the from she quout lik sit a core, on she she on sith the fithive of or the cound its alice rut lith very offs down withit it thourter a fall, arom, this shat one of think\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}